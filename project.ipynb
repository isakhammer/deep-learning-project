{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/isakhammer/deep_learning_project/blob/master/project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "DW4u3R1RHyVJ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from copy import deepcopy as copy \n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Prerequisite**\n",
    " To make this work should this note be launched in the gi repositorub.com/isakhammer/deep_learning_project or have \"project_2_trajectories\" https://github.com/isakhammer/deep_learning_project or have \"project_2_trajectories\" folder in the same repo\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing Training data**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "        \n",
    "\n",
    "def import_batches():\n",
    "    n_batches = 49\n",
    "    \n",
    "    data_prefix = \"datalist_batch_\"\n",
    "    data_path = os.path.join( os.path.join(os.getcwd()), \"project_2_trajectories\")\n",
    "    \n",
    "    batches = {}\n",
    "    \n",
    "    for i in range(n_batches):\n",
    "        # assemble track import path\n",
    "        batch_path = os.path.join(data_path, data_prefix + str(i) + \".csv\")\n",
    "        batch_data = np.loadtxt(batch_path, delimiter=',', skiprows=1)\n",
    "        \n",
    "        # np.newaxis is adding a dimension such that (I,) -> (I, 1)\n",
    "        batch = {}\n",
    "        #batch[\"t\"] = batch_data[:, 0, np.newaxis]\n",
    "        batch[\"Y_q\"] = batch_data[:, 1:4].T\n",
    "        batch[\"Y_p\"] = batch_data[:, 4:7].T\n",
    "        batch[\"c_p\"] = batch_data[:, 7, np.newaxis] \n",
    "        batch[\"c_q\"] = batch_data[:, 8, np.newaxis] # potential energy\n",
    "        \n",
    "        batches[i] = batch\n",
    "\n",
    "    return batches\n",
    "\n",
    "def import_one_batch():\n",
    "   \n",
    "    data_prefix = \"datalist_batch_\"\n",
    "    data_path = os.path.join(os.path.dirname(__file__), \"project_2_trajectories\")\n",
    "    \n",
    "    batches = {}\n",
    "    \n",
    "    i = 0\n",
    "    # assemble track import path\n",
    "    batch_path = os.path.join(data_path, data_prefix + str(i) + \".csv\")\n",
    "    batch_data = np.loadtxt(batch_path, delimiter=',', skiprows=1)\n",
    "        \n",
    "    # np.newaxis is adding a dimension such that (I,) -> (I, 1)\n",
    "    batch = {}\n",
    "    #batch[\"t\"] = batch_data[:, 0, np.newaxis]\n",
    "    batch[\"Y_q\"] = batch_data[:, 1:4].T\n",
    "    batch[\"Y_p\"] = batch_data[:, 4:7].T\n",
    "    batch[\"c_p\"] = batch_data[:, 7, np.newaxis] \n",
    "    batch[\"c_q\"] = batch_data[:, 8, np.newaxis] # potential energy\n",
    "        \n",
    "    batches[0] = batch\n",
    "\n",
    "    return batch\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jTCTpLF4HIom"
   },
   "source": [
    "**1. Implement functions for generating synthetic input data**\n",
    "\n",
    "The proposed functions we choose as synthetic input this functions.\n",
    "\n",
    "$$\n",
    "f_1(y) = \\frac{1}{2}y_1^2 + \\frac{1}{2}y_2^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "f_2(y) = \\frac{1}{2}\\|{y}\\|^2 \n",
    "$$\n",
    "\n",
    "$$\n",
    "f_3(y) = 1 - cos(y)\n",
    "$$\n",
    "\n",
    "$$\n",
    "f_4(y) = \\frac{1}{\\|{y}\\|}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "gDwMTPvwHQDf"
   },
   "outputs": [],
   "source": [
    "def f_1(y):\n",
    "    return 0.5*y[0]**2 + 0.5*y[1]**2 \n",
    "\n",
    "def f_2(y):\n",
    "    return 0.5* np.square(y)\n",
    "\n",
    "def f_3(y):\n",
    "    return 1 - np.cos(y)\n",
    "\n",
    "def f_4(y):\n",
    "    return -1/np.sqrt(y[0]**2 +  y[1]**2 )\n",
    "\n",
    "\n",
    "def generate_synthetic_batches(I,func = \"2sqr\", low=None, high=None):\n",
    "    \n",
    "    batch = {} \n",
    "    \n",
    "    if func == \"2sqr\":\n",
    "        \n",
    "        d_0 = 2\n",
    "        if (high==None) and (low==None):\n",
    "            high=2\n",
    "            low=-2\n",
    "        batch[\"Y\"] = np.random.uniform(high, low, size=(d_0,I) )    \n",
    "        batch[\"c\"] = f_1(batch[\"Y\"])\n",
    "        batch[\"c\"] = batch[\"c\"][:, np.newaxis]\n",
    "        \n",
    "        ct = f_1(batch[\"Y\"] ) \n",
    "        return batch\n",
    "    \n",
    "    elif func == \"1sqr\":\n",
    "        d_0 = 1\n",
    "        \n",
    "        if (high==None) and (low==None):\n",
    "            high=2\n",
    "            low=-2\n",
    "        \n",
    "        batch[\"Y\"] = np.random.uniform(high, low, size=(d_0,I) )\n",
    "        batch[\"c\"] = f_2(batch[\"Y\"] )\n",
    "        batch[\"c\"] = batch[\"c\"].T\n",
    "        \n",
    "        return batch\n",
    "    \n",
    "    elif func == \"1cos\":\n",
    "        d_0 = 1\n",
    "        \n",
    "        if (high==None) and (low==None):\n",
    "            high=np.pi/3\n",
    "            low=-np.pi/3\n",
    "        \n",
    "        batch[\"Y\"] = np.random.uniform(high, low, size=(d_0,I) )\n",
    "        batch[\"c\"] = f_3(batch[\"Y\"] )\n",
    "        batch[\"c\"] = batch[\"c\"].T\n",
    "        return batch\n",
    "    \n",
    "    elif func == \"2norm-1\":\n",
    "        if (high==None) and (low==None):\n",
    "            high=2\n",
    "            low=-2\n",
    "            \n",
    "        d_0 = 2\n",
    "        batch[\"Y\"] = np.random.uniform(high, low, size=(d_0,I))\n",
    "        \n",
    "        for y in batch[\"Y\"].T:\n",
    "            if (np.all(y == 0)):\n",
    "                y = np.array([0.1,0.1])\n",
    "        \n",
    "        batch[\"c\"] = f_4(batch[\"Y\"]) \n",
    "        batch[\"c\"] = batch[\"c\"].T\n",
    "        batch[\"c\"] = batch[\"c\"][:, np.newaxis]\n",
    "            \n",
    "        return batch\n",
    "        \n",
    "    \n",
    "    else:\n",
    "        raise Exception(\"Not axeped func\")\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bMteEKZ0RDug"
   },
   "source": [
    "**TODO**\n",
    "Show phaseplot or something (ot maybe not) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mOUesxX5Ku5E"
   },
   "source": [
    "**2. Implement the neural network for training approximation of Hamiltonian function**\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0PodXYaWSJ1d"
   },
   "source": [
    "The proposed model is formulated as \n",
    "$$\n",
    "\\tilde{F}(Y; \\theta) = \\eta((Z^{(K)})^T w + \\mu \\mathbf{1} ) \n",
    "$$\n",
    "where for $k = 0, 1, 2, ... , K-1 $\n",
    "$$\n",
    "Z^{(k+1)} = Z^{(k)} + h  \\sigma(W_k Z^{(k)} + b_1 ), \\\\\n",
    "Z^{(0)} = \\hat{I} Y.\n",
    "$$\n",
    "Here is a\n",
    "\n",
    "$$\n",
    "\\hat{I} = \n",
    "\\begin{bmatrix}\n",
    "I_{d_0 \\times d_0} \\\\\n",
    "\\textbf{0} \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "where the gradient is define as \n",
    "...\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "0kCPhfZlZUGM"
   },
   "outputs": [],
   "source": [
    "def F_tilde(Y, th, d_0, d, K, h):\n",
    "    \n",
    "    Z = {}\n",
    "    I_d = np.identity(d)[:,:d_0]\n",
    "    Z[0] = I_d@Y\n",
    "\n",
    "    for k in range(K):\n",
    "        Z_hat = th[\"W\"][k]@Z[k]+th[\"b\"][k]\n",
    "        Z[k+1] = Z[k] + h*sigma(Z_hat, False)\n",
    "    \n",
    "    Upsilon = eta(Z[K].T@th[\"w\"]+th[\"mu\"])\n",
    "    \n",
    "    return Z, Upsilon \n",
    "\n",
    "\n",
    "def initialize_weights(d_0, d, K):\n",
    "    th = {}\n",
    "    \n",
    "    th[\"W\"] = np.zeros((K, d, d))\n",
    "    th[\"b\"] = np.zeros((K, d, 1))\n",
    "    \n",
    "    for i in range(K):\n",
    "        th[\"W\"][i] = np.identity(d)\n",
    "        th[\"b\"][i] = np.zeros((d, 1))\n",
    "            \n",
    "    th[\"w\"] = np.ones((d, 1 ))\n",
    "    th[\"mu\"] = np.zeros((1, 1))\n",
    "    \n",
    "    return th\n",
    "\n",
    "\n",
    "\n",
    "def sigma(x, derivative=False):   \n",
    "    if (derivative):\n",
    "        return 1 / np.cosh(x)**2 \n",
    "    return np.tanh(x)\n",
    "\n",
    "def eta(x, derivative=False, identity=False):\n",
    "    if identity==True:\n",
    "        if (derivative):\n",
    "            return np.ones(x.shape)\n",
    "        return x\n",
    "    else:\n",
    "        if (derivative):\n",
    "            return 0.25*(np.cosh(0.5*x) )**(-2)\n",
    "        return 0.5*np.tanh(0.5*x) + 0.5\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UsS4_c-RZMM1"
   },
   "source": [
    "\n",
    "The objective function is on the form \n",
    "\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{2} \\lVert  \\tilde{F} (Y; \\theta) - c \\rVert \n",
    "$$\n",
    "where $\\theta  = \\{ W_0, ..., W_{K-1}, b_0, ..., b_{K-1} , w, \\mu\\}$.\n",
    "\n",
    "However, to be able to optimize the parameters $\\theta$ can the gradient of $\\nabla J(\\theta)$ be formulated for the last layer like this\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial J}{\\partial \\mu} &= \\eta '((Z^{(K)})^T w + \\mu \\mathbf{1} )^T (\\Upsilon -c) \\\\\n",
    "\\frac{\\partial J}{\\partial w} &= Z^{(K)} \\left [ (\\Upsilon - c) \\odot \\eta'((Z^{(K)})^T w + \\mu  \\right ]\n",
    "\\end{align}\n",
    "$$\n",
    "and similarly for the hidden layers,\n",
    "$$\n",
    "\\begin{align}\n",
    "P^{(K)} &= w \\cdot \\left [ (\\Upsilon - c) \\odot \\eta'\\left (  (Z^{(K)})^T w + \\mu \\mathbf{1}\\ \\right) \\right ]^T  \\\\ \n",
    "P^{(k-1)} &= P^{(k)} + h W_{k-1}^{T} \\cdot \\left [ \\sigma'\\left (  W_{k-1} Z^{(k-1)} + \\beta_{k-1} \\ \\right) \\odot P^{(k)} \\right ]  \\\\\n",
    "\\frac{\\partial J}{\\partial W_k} &= h \\left [ P^{(k+1)} \\odot \\sigma ' (W_k Z^{(k)}  + b_k) \\right ] (Z^{(k)})^T \\\\\n",
    "\\frac{\\partial J}{\\partial b_k} &= h \\left [ P^{(k+1)} \\odot \\sigma ' (W_k Z^{(k)} + b_k) \\right ] \\mathbf{1}.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "zS8S6LqDkkML"
   },
   "outputs": [],
   "source": [
    "def J_func(Upsilon, c):\n",
    "    return 0.5*np.linalg.norm(c - Upsilon)**2\n",
    "\n",
    "\n",
    "def dJ_func(c, Y, th, d_0, d, K, h):\n",
    "    Z, Upsilon = F_tilde(Y, th, d_0, d, K, h)\n",
    "    I = Upsilon.shape[0]\n",
    "        \n",
    "    etahat = eta(Z[K].T@th[\"w\"] + th[\"mu\"]*np.ones(( I, 1)), derivative=True )\n",
    "        \n",
    "    P = np.zeros(( K+1, d, I))\n",
    "    \n",
    "    P[K] = np.outer(th[\"w\"], ( (Upsilon - c)* etahat).T)\n",
    "        \n",
    "    dJ_mu = etahat.T @(Upsilon - c)\n",
    "        \n",
    "    dJ_w = Z[K] @ ((Upsilon - c) * etahat)\n",
    "        \n",
    "    for k in range(K, 0, -1):\n",
    "        P[k-1] = P[k] + h*th[\"W\"][k-1].T @ (sigma(th[\"W\"][k-1]@Z[k-1]+np.outer(th[\"b\"][k-1],np.ones(I)), True) * P[k])\n",
    "            \n",
    "    dJ_W = np.zeros((K, d, d))\n",
    "    dJ_b = np.zeros((K, d, 1))\n",
    "        \n",
    "    for k in range(K):\n",
    "        dsigma = sigma(th[\"W\"][k]@Z[k]+np.outer(th[\"b\"][k],np.ones(I)),True)\n",
    "            \n",
    "        dJ_W[k] = h*(P[k+1]*dsigma) @ Z[k].T\n",
    "        dJ_b[k] = (h*(P[k+1]*dsigma) @ np.ones(I))[:,np.newaxis]\n",
    "    dJ = {}\n",
    "    dJ[\"w\"], dJ[\"mu\"], dJ[\"W\"], dJ[\"b\"] = dJ_w, dJ_mu, dJ_W, dJ_b\n",
    "    return dJ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DSmD03R5kwH_"
   },
   "source": [
    "To simplify input and output and make the training process more convinient are we introducing a linear scale function \n",
    "$$\n",
    "\\hat{x} = \\frac{(x + \\alpha )b - (x - \\beta)a }{b - a }\n",
    "$$\n",
    "\n",
    "and the inverse function\n",
    "\n",
    "$$\n",
    "x = \\frac{(\\hat{x}+ \\alpha ) b - (\\hat{x} - \\beta)a}{ \\beta -  \\alpha}\n",
    "$$\n",
    "\n",
    "where $b = \\max _{i,j} x$ and $a = \\min _{i,j} x$ such that all values in $ [a,b ] \\mapsto [\\alpha, \\beta]$. Usually will we choose $\\beta = 1$ and $\\alpha = 1$. In addition to this have we chosen to do scaling with no shift. Later in the project will we also have use for inversion of scale with no shift, this is in the context of finding the derivative. \n",
    "$$\n",
    "\\hat{x} = \\frac{x (b-a) }{\\beta - \\alpha }\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "I_MsdgkmsSgh"
   },
   "outputs": [],
   "source": [
    "\n",
    "def scale(x, alpha=0, beta=1, returnParameters = False):\n",
    "    \n",
    "    a = np.min(x)\n",
    "    b = np.max(x)\n",
    "    \n",
    "    if returnParameters:        \n",
    "        return alpha, beta, a, b\n",
    "\n",
    "    else:\n",
    "        def  invscale(x):\n",
    "            return ((x + alpha)*b - (x - beta)*a) / (beta-alpha)\n",
    "        \n",
    "        return ( (b - x)*alpha + (x - a)*beta)/(b - a), invscale\n",
    "     \n",
    "def invscaleparameter(x, alpha, beta, a, b):\n",
    "    return ((x+alpha)*b - (x-beta)*a) / (beta-alpha)\n",
    "\n",
    "def invscaleparameter_no_shift(x, alpha, beta, a, b):\n",
    "    return x*(b-a)/(beta-alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "srOtimIQsWbx"
   },
   "source": [
    "**Optimization Algorithms**\n",
    "\n",
    "We will evaluate two optimization algorithms in the project. \n",
    "\n",
    "**Gradient Descent Algorithm**\n",
    "$$\n",
    "\\theta_{i+1} = \\theta_{i+1} - \\tau \\nabla J(\\theta^{(r)})\n",
    "$$\n",
    "**Adams Algorithm**\n",
    "\n",
    "```\n",
    "beta_1, beta_2 = 0.9, 0.999\n",
    "alpha = 0.01\n",
    "epsilon = 10^(-8)\n",
    "\n",
    "v_0, m_0 = 0, 0\n",
    "\n",
    "while not converged:\n",
    "    g = dJ(th)\n",
    "    m = beta_1 m + ( 1- beta_1)g_j\n",
    "    v = beta_2 v + (1 - beta_2)(g*g)\n",
    "    m_hat = m/(1 - beta_1^j)\n",
    "    v_hat = v/(1 - beta_2^j)\n",
    "    theta = theta - alpha m_hat/ ( v_hat^(0.5) + epsilon )\n",
    "    \n",
    "```\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "r5gKuxdC1rNS"
   },
   "outputs": [],
   "source": [
    "\n",
    "def gradientDesent(K, th, dJ, tau):\n",
    "    \n",
    "    th[\"mu\"] = th[\"mu\"] - tau*dJ[\"mu\"]\n",
    "    th[\"w\"] = th[\"w\"] - tau*dJ[\"w\"]\n",
    "    \n",
    "    th[\"W\"] = th[\"W\"] -  tau*dJ[\"W\"]\n",
    "    th[\"b\"] = th[\"b\"] -  tau*dJ[\"b\"]\n",
    "    return th\n",
    "\n",
    "\n",
    "def adam_algebra(th, dJ, v, m, key, j, alpha =10**(-5)):\n",
    "        beta_1, beta_2 =  0.9, 0.999\n",
    "        epsilon =  10**(-8)\n",
    "    \n",
    "        g = dJ[key] \n",
    "        m[key] = beta_1*m[key] + (1- beta_1)*g\n",
    "        v[key] = beta_2*v[key] + (1 - beta_2)*(g*g)\n",
    "        mhat = m[key]/(1 - beta_1**(j+1))\n",
    "        vhat = v[key]/(1 - beta_2**(j+1))\n",
    "        th[key] -= alpha*mhat/(np.square(vhat) + epsilon)\n",
    "        return th, v, m\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F4GW6GbV1xfV"
   },
   "source": [
    "**Training Procedure**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "0MSDfOEQLAES",
    "outputId": "eb6eb2a6-1d5b-435f-b658-7ce5a139b6e6"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def train(c, d, d_0, K, h, Y, th, tau=0.0005, max_it=60, print_it=True, method=\"gd\", alpha =7.5*10**(-5)):\n",
    "    # compute Zk\n",
    "    err = np.inf\n",
    "    tol = 0.01\n",
    "    \n",
    "    \n",
    "    itr = 0\n",
    "    Z, Upsilon = F_tilde(Y, th, d_0, d, K, h)\n",
    "    JJ = np.zeros(max_it+1)\n",
    "    err = J_func(Upsilon,c)\n",
    "    \n",
    "    JJ[0] = err\n",
    "    \n",
    "    # Adam parameters \n",
    "    m = {}\n",
    "    m[\"mu\"] = np.zeros(th[\"mu\"].shape)\n",
    "    m[\"w\"] = np.zeros(th[\"w\"].shape)\n",
    "    m[\"W\"] = np.zeros(th[\"W\"].shape)\n",
    "    m[\"b\"] = np.zeros(th[\"b\"].shape)\n",
    "    v = copy(m)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    while (itr < max_it ):\n",
    "        \n",
    "        Z, Upsilon = F_tilde(Y, th, d_0, d, K, h)\n",
    "        \n",
    "        \n",
    "        \n",
    "        if (method==\"gd\"):\n",
    "            dJ = dJ_func(c, Y, th, d_0, d, K, h)\n",
    "            th = gradientDesent(K, th, dJ, tau)\n",
    "        \n",
    "        elif (method==\"adam\"):\n",
    "            j = itr\n",
    "            \n",
    "            dJ = dJ_func(c, Y, th, d_0, d, K, h)\n",
    "            \n",
    "            th, v, m = adam_algebra(th, dJ, v, m, \"mu\", j, alpha)\n",
    "            th, v, m = adam_algebra(th, dJ, v, m, \"w\", j, alpha)\n",
    "            th, v, m = adam_algebra(th, dJ, v, m, \"W\", j, alpha)\n",
    "            th, v, m = adam_algebra(th, dJ, v, m, \"b\", j, alpha)\n",
    "            \n",
    "        else:\n",
    "            print(\"No optimization method\")\n",
    "        \n",
    "        err = J_func(Upsilon, c)  \n",
    "        \n",
    "        JJ[itr+1] = err\n",
    "        \n",
    "        itr += 1\n",
    "        \n",
    "        if(itr%600 == 0) and (print_it == True):\n",
    "            print(itr,err)\n",
    "        \n",
    "    return JJ , th\n",
    "        \n",
    "def stocgradient(c, d, d_0, K, h, Y, th, tau, max_it , bsize, sifts = 100, save = False, savefile = \"\"):\n",
    "    \n",
    "    JJ = np.zeros(sifts)\n",
    "    I = Y.shape[1]\n",
    "    totitr = int(I/bsize)\n",
    "    for siftnum in range(sifts):\n",
    "        print(siftnum)\n",
    "        \n",
    "        indexes = np.array(range(I))\n",
    "        \n",
    "        np.random.shuffle(indexes)\n",
    "        \n",
    "        itr = 0\n",
    "        \n",
    "        Z, Upsilon = F_tilde(Y, th, d_0, d, K, h)\n",
    "        err = J_func(Upsilon, c)\n",
    "        JJ[siftnum] = err\n",
    "        \n",
    "        if save and siftnum%100 == 0:\n",
    "            th_file = open(savefile, \"wb\")\n",
    "            pickle.dump(th, th_file)\n",
    "            th_file.close()\n",
    "        \n",
    "        while len(indexes) > 0:\n",
    "            \"\"\"\n",
    "            if (itr % 50 == 0):\n",
    "                print(siftnum,itr,totitr)\n",
    "            itr +=1\n",
    "            \"\"\"\n",
    "            if len(indexes) >= bsize:\n",
    "                #bsliceI = np.random.choice( indexes, bsize, False)\n",
    "                bsliceI = indexes[:bsize]\n",
    "                Yslice = Y[:,bsliceI]\n",
    "                cslice = c[bsliceI]\n",
    "                \n",
    "                dJJ, th = train(cslice, d, d_0, K, h, Yslice, th, tau, max_it)\n",
    "                \n",
    "                #JJ = np.append(JJ,dJJ)\n",
    "                \n",
    "                indexes = indexes[bsize:]\n",
    "                \n",
    "                \n",
    "            else:\n",
    "                Yslice = Y[:,indexes]\n",
    "                cslice = c[indexes]\n",
    "                \n",
    "                \n",
    "                dJJ, th = train(cslice, d, d_0, K, h, Yslice, th, tau, max_it)\n",
    "                \n",
    "                #JJ = np.append(JJ,dJJ)\n",
    "                \n",
    "                indexes = []\n",
    "            \n",
    "    return JJ, th\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YRKUdOVI2Pql"
   },
   "source": [
    "**Stochastic Gradient**\n",
    "\n",
    "Since the objective function is independent of time can it be training with unordered trainingdata. Using this fact can we sweep through the dataset faster by random shuffling and slizing the entire dataset.  We will in the implementation assume that the dataset is already merged for $c$ and $Y$, and will allocate the job to the data pre-processing. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "TbYHk2LX2s1w"
   },
   "outputs": [],
   "source": [
    "def stocgradient(c, d, d_0, K, h, Y, th, tau, max_it , bsize, sifts = 100):\n",
    "    \n",
    "    JJ = np.array([])\n",
    "    I = Y.shape[1]\n",
    "    totitr = int(I/bsize)\n",
    "    for siftnum in range(sifts):\n",
    "        \n",
    "        indexes = np.array(range(I))\n",
    "        np.random.shuffle(indexes)\n",
    "        \n",
    "        itr = 0\n",
    "        Z, Upsilon = F_tilde(Y, th, d_0, d, K, h)\n",
    "        err = J_func(Upsilon, c)\n",
    "        JJ = np.append(JJ, err)\n",
    "        \n",
    "        while len(indexes) > 0:\n",
    "            \n",
    "            if len(indexes) >= bsize:\n",
    "                bsliceI = indexes[:bsize]\n",
    "                Yslice = Y[:,bsliceI]\n",
    "                cslice = c[bsliceI]\n",
    "                \n",
    "                dJJ, th = train(cslice, d, d_0, K, h, Yslice, th, tau, max_it)\n",
    "                indexes = indexes[bsize:]\n",
    "             \n",
    "            else:\n",
    "                Yslice = Y[:,indexes]\n",
    "                cslice = c[indexes]\n",
    "                \n",
    "                dJJ, th = train(cslice, d, d_0, K, h, Yslice, th, tau, max_it) \n",
    "                indexes = []\n",
    "            \n",
    "    return JJ, th\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bdoRivM2NrQ5"
   },
   "source": [
    "(a) Test the model by using the suggested functions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "MIwq2aMhOFon",
    "outputId": "56b16abd-4e4f-4ba2-d8b6-440fb5016b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n"
     ]
    },
    {
     "ename": "EOFError",
     "evalue": "Ran out of input",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-37e026ba9c4d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0mtrain_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"1sqr\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m \u001b[0mtest_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"1sqr\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-23-37e026ba9c4d>\u001b[0m in \u001b[0;36mtest_func\u001b[0;34m(func)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0minv_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"_inv.pkl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     \u001b[0minv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minv_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m     \u001b[0minv_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mEOFError\u001b[0m: Ran out of input"
     ]
    }
   ],
   "source": [
    "def train_func(func):\n",
    "    \n",
    "    \n",
    "    \n",
    "    data = generate_synthetic_batches(I, func)\n",
    "    \n",
    "    Y =data[\"Y\"]\n",
    "    c = data[\"c\"]\n",
    "    sc , invc = scale(c)\n",
    "    sparameters = scale(c,returnParameters = True)\n",
    "    \n",
    "    inv_file = open( func+\"_inv.pkl\", \"wb\")\n",
    "    pickle.dump(sparameters, inv_file)\n",
    "    inv_file.close()\n",
    "    \n",
    "    d_0 = Y.shape[0]\n",
    "    d = d_0*2\n",
    "    \n",
    "    th = initialize_weights(d_0, d, K)\n",
    "    \n",
    "    JJ, th = stocgradient(sc, d, d_0, K, h, Y, th, tau, 1 , Ihat, sifts)\n",
    "    \n",
    "    plt.plot(JJ)\n",
    "    plt.yscale(\"log\")\n",
    "    plt.show()\n",
    "    \n",
    "    th_file = open(func + \"_th.pkl\", \"wb\")\n",
    "    pickle.dump(th, th_file)\n",
    "    th_file.close()\n",
    "    \n",
    "    \n",
    "    \n",
    "def test_func(func):\n",
    "    \n",
    "    numData = 2000\n",
    "    \n",
    "    K = 20\n",
    "    h = 0.1\n",
    "    d_0 = 1\n",
    "    d = 2\n",
    "    \n",
    "    if func == \"1sqr\":\n",
    "        Y  = np.linspace(-2,2,numData)\n",
    "        Y = Y[:,np.newaxis].T\n",
    "        c = 1/2*Y**2\n",
    "        c = c.T\n",
    "        \n",
    "    elif func == \"1cos\":\n",
    "        Y = np.linspace(-np.pi/3,np.pi/3,numData)\n",
    "        Y = Y[:,np.newaxis].T\n",
    "        c = 1-np.cos(Y)\n",
    "        c = c.T\n",
    "        \n",
    "    else:\n",
    "        raise Exception(\"No func\")\n",
    "    \n",
    "    \n",
    "    inv_file = open( func+\"_inv.pkl\", \"rb\")\n",
    "    inv = pickle.load(inv_file)\n",
    "    inv_file.close()\n",
    "    \n",
    "    th_file = open(func + \"_th.pkl\", \"rb\")\n",
    "    th = pickle.load(w_file)\n",
    "    w_file.close()\n",
    "    z, yhat = F_tilde(Y, th, d_0, d, K, h)\n",
    "    \n",
    "    y = invscaleparameter(yhat, inv[0], inv[1], inv[2], inv[3])\n",
    "    \n",
    "    plt.plot(Y.T,y, label =\"y\")\n",
    "    plt.plot(Y.T,c, label =\"c\")\n",
    "    plt.title(\"Comparison for \" + func)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "I = 500\n",
    "K = 20\n",
    "h = 0.1\n",
    "sifts = 2400\n",
    "Ihat = 320\n",
    "tau = 3/Ihat    \n",
    "    \n",
    "\n",
    "do_training = False\n",
    "\n",
    "print(\"Hello\")\n",
    "\n",
    "if (do_training):\n",
    "    print(\"Starts training\")\n",
    "    train_func(func=\"1sqr\")\n",
    "\n",
    "test_func(func=\"1sqr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "spXY0gxgVEq6"
   },
   "source": [
    "(b) Investigate systematically what are optimal choices for K, Ï„ , d, h and any\n",
    "other choices you need to make. Balance performance in the generalisation\n",
    "phase with time consumption of training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d5Fb1mimJs38"
   },
   "source": [
    "**Tau Sensitivity**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "AfNOhcBwJkCh",
    "outputId": "8e2fdb49-b4d1-4b9f-91e4-56ee52f631ab"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Chosen default values\n",
    "K = 20\n",
    "h = 0.1\n",
    "d_0 = 2\n",
    "d = 4\n",
    "I = 600\n",
    "max_it = 300\n",
    "tau = 0.1\n",
    "\n",
    "\n",
    "def tau_sensitivity(method=\"gd\"):\n",
    "           \n",
    "    I = 100     \n",
    "    b = generate_synthetic_batches(I)\n",
    "    \n",
    "    c, inv = scale(b[\"c\"])\n",
    "    Y = b[\"Y\"]\n",
    "    #Y = scale(b[\"Y\"])\n",
    "    d_0 = Y.shape[0]\n",
    "    \n",
    "    var = [ 2, 1, 0.5, 0.25, 0.1, 0.01]\n",
    "    it = np.arange(0,max_it+1)\n",
    "    \n",
    "    for i in range(len(var)):    \n",
    "        tau = var[i]\n",
    "        th = initialize_weights(d_0, d, K)\n",
    "        JJ,th = train(c, d, d_0, K, h, Y, th, tau=tau, max_it=max_it, method=method)\n",
    "        plt.plot(it, JJ, label=\"tau: \"+ str(tau))\n",
    "    \n",
    "    plt.title(\"Tau Sensitivity Analysis for \" + method)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "def tauI_sensitivity(I, method=\"gd\"):            \n",
    "    \n",
    "    b = generate_synthetic_batches(I)\n",
    "    \n",
    "    c, inv = scale(b[\"c\"])\n",
    "    Y = b[\"Y\"]\n",
    "    #Y = scale(b[\"Y\"])\n",
    "    d_0 = Y.shape[0]\n",
    "    \n",
    "    var = np.array([ 40, 20, 10, 5, 2, 0.2])/I\n",
    "    it = np.arange(0,max_it+1)\n",
    "    \n",
    "    for i in range(len(var)):    \n",
    "        tau = var[i]\n",
    "        th = initialize_weights(d_0, d, K)\n",
    "        JJ,th = train(c, d, d_0, K, h, Y, th, tau=tau, max_it=max_it, method=method)\n",
    "        plt.plot(it, JJ, label=\"tau: \"+ str(tau) + \"/I\")\n",
    "    \n",
    "    plt.title(\"Tau Sensitivity Analysis for \" + method + \", I: \"+str(I))\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "tau_sensitivity(method=\"gd\")\n",
    "I = [160, 320, 720, 1440, 2880, 5760]\n",
    "for i in I:\n",
    "    tauI_sensitivity(i)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lu7F-HR1Jy8_"
   },
   "source": [
    "**Alpha Sensitivity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "i-vkjCabJqMm",
    "outputId": "2045373c-47e5-403a-edcd-531730c57b37"
   },
   "outputs": [],
   "source": [
    "\n",
    "def alpha_sensitivity(method=\"adam\"):\n",
    "                \n",
    "    I = 100\n",
    "    b = generate_synthetic_batches(I)\n",
    "    \n",
    "    c, inv = scale(b[\"c\"])\n",
    "    Y = b[\"Y\"]\n",
    "    #Y = scale(b[\"Y\"])\n",
    "    d_0 = Y.shape[0]\n",
    "    \n",
    "    var = [0.75*10**-4, 0.5*10**-4, 0.35*10**-4, 0.75*10**-5, 0.5*10**-5, 0.25*10**-6]\n",
    "    it = np.arange(0,max_it+1)\n",
    "    \n",
    "    for i in range(len(var)):    \n",
    "        alpha = var[i]\n",
    "        th = initialize_weights(d_0, d, K)\n",
    "        JJ,th = train(c, d, d_0, K, h, Y, th, tau=tau, max_it=max_it, method=method, alpha=alpha)\n",
    "        plt.plot(it, JJ, label=\"alpha: \"+ str(alpha))\n",
    "    \n",
    "    plt.title(\"Alpha Sensitivity Analysis for \" + method)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "def alphaI_sensitivity(I, method=\"adam\"):            \n",
    "    \n",
    "    b = generate_synthetic_batches(I)\n",
    "    \n",
    "    c, inv = scale(b[\"c\"])\n",
    "    Y = b[\"Y\"]\n",
    "    d_0 = Y.shape[0]\n",
    "    \n",
    "    var = np.array( [10**-1, 10**-2, 10**-3, 10**-4, 10**-5, 10**-6 ])#/I\n",
    "    it = np.arange(0,max_it+1)\n",
    "    \n",
    "    for i in range(len(var)):    \n",
    "        alpha = var[i]\n",
    "        th = initialize_weights(d_0, d, K)\n",
    "        JJ,th = train(c, d, d_0, K, h, Y, th, tau=tau, max_it=max_it, method=method, alpha=alpha)\n",
    "        plt.plot(it, JJ, label=\"alpha: \"+ str(alpha) )\n",
    "    \n",
    "    plt.title(\"ADAM Sensitivity - \" + \"I: \"+str(I))\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Cost function\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "I = [160, 320, 720, 1440, 2880, 5760]\n",
    "for i in I:\n",
    "    alphaI_sensitivity(i)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VSCYZtYBJ_Q3"
   },
   "source": [
    "**h Sensitivity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "O58x1zChNtGe",
    "outputId": "9cc50763-e47c-47d9-c4f0-886413e54c2c"
   },
   "outputs": [],
   "source": [
    "\n",
    "def h_sensitivity(method=\"gd\"):\n",
    "    I = 300\n",
    "    b = generate_synthetic_batches(I)\n",
    "    \n",
    "    c, inv = scale(b[\"c\"])\n",
    "    Y = b[\"Y\"]\n",
    "    #Y = scale(b[\"Y\"])\n",
    "    d_0 = Y.shape[0]\n",
    "    \n",
    "    var = var = [ 0.14, 0.12, 0.1, 0.07, 0.05, 0.01]\n",
    "    it = np.arange(0,max_it+1)\n",
    "    \n",
    "    for i in range(len(var)):    \n",
    "        h = var[i]\n",
    "        th = initialize_weights(d_0, d, K)\n",
    "        JJ,th = train(c, d, d_0, K, h, Y, th, tau=tau, max_it=max_it, method=method)\n",
    "        plt.plot(it, JJ, label=\"h: \"+ str(var[i]))\n",
    "    \n",
    "    plt.title(\"h Sensitivity Analysis for \" + method)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "h_sensitivity(method=\"gd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JjsPTvJWKI78"
   },
   "source": [
    "**I-sensitivity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 422
    },
    "id": "Q5iSsxhkN1cN",
    "outputId": "19e5621d-16f6-4128-9744-960669775d52"
   },
   "outputs": [],
   "source": [
    "\n",
    "def I_sensitivity(method=\"gd\"):\n",
    "    max_it = 3000             \n",
    "    var = var = [5, 10, 15, 20, 40, 80, 160, 320]\n",
    "    it = np.arange(0,max_it+1)\n",
    "    \n",
    "    for i in range(len(var)):    \n",
    "        I = var[i]\n",
    "        b = generate_synthetic_batches(I)\n",
    "        c, inv = scale(b[\"c\"])\n",
    "        Y = b[\"Y\"]\n",
    "        #Y = scale(b[\"Y\"])\n",
    "        d_0 = Y.shape[0]\n",
    "    \n",
    "        print(\"I:\", I)\n",
    "        th = initialize_weights(d_0, d, K)\n",
    "        JJ,th = train(c, d, d_0, K, h, Y, th, tau=tau, max_it=max_it, print_it=False, method=method)\n",
    "        JJ = JJ/JJ[0]\n",
    "        plt.plot(it, JJ, label=\"I: \"+ str(var[i]))\n",
    "    \n",
    "    #plt.yscale(\"log\")\n",
    "    plt.title(\"I Sensitivity Analysis for \" + method)\n",
    "    plt.yscale(\"log\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "I_sensitivity(method=\"gd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "51B3HXphKQE3"
   },
   "source": [
    "**d-sensitivity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "AS3-TqQxNxDf",
    "outputId": "14e6f9e1-549c-4300-ab2b-ef3a341c2409"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def d_sensitivity(method=\"gd\"):\n",
    "    I = 300         \n",
    "    b = generate_synthetic_batches(I)\n",
    "    \n",
    "    c, inv = scale(b[\"c\"])\n",
    "    Y = b[\"Y\"]\n",
    "    #Y = scale(b[\"Y\"])\n",
    "    d_0 = Y.shape[0]\n",
    "    \n",
    "    var = var = [ 2, 3, 4, 5, 6, 7, 8, 10 ]\n",
    "    it = np.arange(0,max_it+1)\n",
    "    \n",
    "    for i in range(len(var)):    \n",
    "        d = var[i]\n",
    "        th = initialize_weights(d_0, d, K)\n",
    "        JJ,th = train(c, d, d_0, K, h, Y, th, tau=tau, max_it=max_it, method=method)\n",
    "        plt.plot(it, JJ, label=\"d: \"+ str(var[i]))\n",
    "    \n",
    "    plt.title(\"d Sensitivity Analysis for \" + method)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "d_sensitivity(method=\"gd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kQrLZ8iJLAjE"
   },
   "source": [
    "**K-Sensitivity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "qGZ6Cu6jK9_L",
    "outputId": "d6772023-5306-4fc3-8950-2314c85becad"
   },
   "outputs": [],
   "source": [
    "\n",
    "def K_sensitivity(method=\"gd\"):\n",
    "    I = 300         \n",
    "    b = generate_synthetic_batches(I)\n",
    "    \n",
    "    c, inv = scale(b[\"c\"])\n",
    "    Y = b[\"Y\"]\n",
    "    #Y = scale(b[\"Y\"])\n",
    "    d_0 = Y.shape[0]\n",
    "    \n",
    "    var = var = [ 4, 6, 10, 14, 17, 20, 30]\n",
    "    it = np.arange(0,max_it+1)\n",
    "    \n",
    "    for i in range(len(var)):    \n",
    "        K = var[i]\n",
    "        th = initialize_weights(d_0, d, K)\n",
    "        JJ,th = train(c, d, d_0, K, h, Y, th, tau=tau, max_it=max_it, method=method)\n",
    "        plt.plot(it, JJ, label=\"K: \"+ str(var[i]))\n",
    "    \n",
    "    plt.title(\"K Sensitivity Analysis for \" + method)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "K_sensitivity(method=\"gd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jSf0GGCDXlT2"
   },
   "source": [
    "(c) Train the model for the case of data given (with unknown Hamiltonian func-\n",
    "tion)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pQNtdoYPXDVi"
   },
   "outputs": [],
   "source": [
    "\n",
    "def train_uknown():\n",
    "    K = 20\n",
    "    h = 0.1\n",
    "    I = 80\n",
    "    max_it = 1\n",
    "    sifts = 110\n",
    "    tau = 0.1\n",
    "    \n",
    "    batches = import_batches()\n",
    "    batch1 = batches[0]\n",
    "    antB = 40\n",
    "    testbatch = batches[antB-1]\n",
    "    \n",
    "    Y = batch1[\"Y_q\"]\n",
    "    d_0 = Y.shape[0]\n",
    "    d = d_0*2\n",
    "    \n",
    "    th = initialize_weights(d_0, d, K)\n",
    "    JJ = np.array([])\n",
    "\n",
    "    bigbatch = {}\n",
    "    bigbatch[\"Y\"] = np.array([[],[],[]])\n",
    "    bigbatch[\"c\"] = np.array([])\n",
    "    \n",
    "    for i in range(antB):\n",
    "        batch = batches[i]\n",
    "        bigbatch[\"Y\"] = np.append(bigbatch[\"Y\"],batch[\"Y_q\"],1)\n",
    "        bigbatch[\"c\"] = np.append(bigbatch[\"c\"],batch[\"c_q\"])\n",
    "        \n",
    "    Y = bigbatch[\"Y\"]\n",
    "    c,inv = scale(bigbatch[\"c\"][:,np.newaxis])\n",
    "    \n",
    "    JJ, th = stocgradient(c, d, d_0, K, h, Y, th, tau, 1 , 40, sifts)\n",
    " \n",
    "    plt.plot(JJ)\n",
    "    plt.yscale(\"log\")\n",
    "    plt.show()\n",
    "    \n",
    "    tY = testbatch[\"Y_q\"]\n",
    "    tc,invscale = scale(testbatch[\"c_q\"])\n",
    "    \n",
    "    z, yhat = F_tilde(tY, th, d_0, d, K, h)\n",
    "    \n",
    "    y = invscale(yhat)\n",
    "    ic = invscale(tc)\n",
    "    \n",
    "    plt.plot(y)\n",
    "    plt.plot(ic)\n",
    "    plt.show()\n",
    "    \n",
    "    th_file = open(\"weights.pkl\", \"wb\")\n",
    "    pickle.dump(th, th_file)\n",
    "    th_file.close()\n",
    "\n",
    "    \n",
    "def test_nlp(pq):\n",
    "    \n",
    "    numData = 2000\n",
    "    \n",
    "    K = 20\n",
    "    h = 0.1\n",
    "    d_0 = 1\n",
    "    d = 2\n",
    "    \n",
    "    if pq == \"p\":\n",
    "        Y  = np.linspace(-2,2,numData)\n",
    "        Y = Y[:,np.newaxis].T\n",
    "        c = 1/2*Y**2\n",
    "        c = c.T\n",
    "        \n",
    "    elif pq == \"q\":\n",
    "        Y = np.linspace(-np.pi/3,np.pi/3,numData)\n",
    "        Y = Y[:,np.newaxis].T\n",
    "        c = 1-np.cos(Y)\n",
    "        c = c.T\n",
    "    else:\n",
    "        raise Exception(\"p or q\")\n",
    "    \n",
    "    \n",
    "    inv_file = open( pq + \"_nlp_inv.pkl\", \"rb\")\n",
    "    inv = pickle.load(inv_file)\n",
    "    inv_file.close()\n",
    "    \n",
    "    w_file = open(pq + \"_nlp_w.pkl\", \"rb\")\n",
    "    th = pickle.load(w_file)\n",
    "    w_file.close()\n",
    "    z, yhat = F_tilde(Y, th, d_0, d, K, h)\n",
    "    \n",
    "    y = invscaleparameter(yhat, inv[0], inv[1], inv[2], inv[3])\n",
    "    \n",
    "    plt.plot(Y.T,y, label =\"y\")\n",
    "    plt.plot(Y.T,c, label =\"c\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "#train_uknown()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPsSxaGZFN9xDiaWOanGzkS",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "project.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
