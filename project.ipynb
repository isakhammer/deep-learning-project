{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPtM//J3edZ5I8mKZcPCIrt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/isakhammer/deep_learning_project/blob/master/project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DW4u3R1RHyVJ"
      },
      "source": [
        "import numpy as np\n",
        "from copy import deepcopy as copy \n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTCTpLF4HIom"
      },
      "source": [
        "**1. Implement functions for generating synthetic input data**\n",
        "\n",
        "\n",
        "$$\n",
        "f_1(y) = \\frac{1}{2}y_1^2 + \\frac{1}{2}y_2^2\n",
        "$$\n",
        "\n",
        "$$\n",
        "f_2(y) = \\frac{1}{2}\\|{y}\\|^2 \n",
        "$$\n",
        "\n",
        "$$\n",
        "f_3(y) = 1 - cos(y)\n",
        "$$\n",
        "\n",
        "$$\n",
        "f_4(y) = \\frac{1}{\\|{y}\\|}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDwMTPvwHQDf"
      },
      "source": [
        "def f_1(y):\n",
        "    return 0.5*y[0]**2 + 0.5*y[1]**2 \n",
        "\n",
        "def f_2(y):\n",
        "    return 0.5* np.square(y)\n",
        "\n",
        "def f_3(y):\n",
        "    return 1 - np.cos(y)\n",
        "\n",
        "def f_4(y):\n",
        "    return -1/np.sqrt(y[0]**2 +  y[1]**2 )\n",
        "\n",
        "\n",
        "def generate_synthetic_batches(I,func = \"2sqr\", low=None, high=None):\n",
        "    \n",
        "    batch = {} \n",
        "    \n",
        "    if func == \"2sqr\":\n",
        "        \n",
        "        d_0 = 2\n",
        "        if (high==None) and (low==None):\n",
        "            high=2\n",
        "            low=-2\n",
        "        batch[\"Y\"] = np.random.uniform(high, low, size=(d_0,I) )    \n",
        "        batch[\"c\"] = f_1(batch[\"Y\"])\n",
        "        batch[\"c\"] = batch[\"c\"][:, np.newaxis]\n",
        "        \n",
        "        ct = f_1(batch[\"Y\"] ) \n",
        "        return batch\n",
        "    \n",
        "    elif func == \"1sqr\":\n",
        "        d_0 = 1\n",
        "        \n",
        "        if (high==None) and (low==None):\n",
        "            high=2\n",
        "            low=-2\n",
        "        \n",
        "        batch[\"Y\"] = np.random.uniform(high, low, size=(d_0,I) )\n",
        "        batch[\"c\"] = f_2(batch[\"Y\"] )\n",
        "        batch[\"c\"] = batch[\"c\"].T\n",
        "        \n",
        "        return batch\n",
        "    \n",
        "    elif func == \"1cos\":\n",
        "        d_0 = 1\n",
        "        \n",
        "        if (high==None) and (low==None):\n",
        "            high=np.pi/3\n",
        "            low=-np.pi/3\n",
        "        \n",
        "        batch[\"Y\"] = np.random.uniform(high, low, size=(d_0,I) )\n",
        "        batch[\"c\"] = f_3(batch[\"Y\"] )\n",
        "        batch[\"c\"] = batch[\"c\"].T\n",
        "        return batch\n",
        "    \n",
        "    elif func == \"2norm-1\":\n",
        "        if (high==None) and (low==None):\n",
        "            high=2\n",
        "            low=-2\n",
        "            \n",
        "        d_0 = 2\n",
        "        batch[\"Y\"] = np.random.uniform(high, low, size=(d_0,I))\n",
        "        \n",
        "        for y in batch[\"Y\"].T:\n",
        "            if (np.all(y == 0)):\n",
        "                y = np.array([0.1,0.1])\n",
        "        \n",
        "        batch[\"c\"] = f_4(batch[\"Y\"]) \n",
        "        batch[\"c\"] = batch[\"c\"].T\n",
        "        batch[\"c\"] = batch[\"c\"][:, np.newaxis]\n",
        "            \n",
        "        return batch\n",
        "        \n",
        "    \n",
        "    else:\n",
        "        raise Exception(\"Not axeped func\")\n",
        "        \n",
        "        \n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMteEKZ0RDug"
      },
      "source": [
        "**TODO**\n",
        "Show phaseplot or something\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tIiqLEHtTQ2s",
        "outputId": "5838bfb1-9621-430e-8734-a29ad9c18e97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "\"\"\"\n",
        "b1 = generate_synthetic_batches(I,func = \"2sqr\")\n",
        "b2 = generate_synthetic_batches(I,func = \"1sqr\")\n",
        "b3 = generate_synthetic_batches(I,func = \"1cos\")\n",
        "b4 = generate_synthetic_batches(I,func = \"2norm-1\")\n",
        "\n",
        "print(b1[\"Y\"].shape, b1[\"c\"].shape)\n",
        "\"\"\""
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nb1 = generate_synthetic_batches(I,func = \"2sqr\")\\nb2 = generate_synthetic_batches(I,func = \"1sqr\")\\nb3 = generate_synthetic_batches(I,func = \"1cos\")\\nb4 = generate_synthetic_batches(I,func = \"2norm-1\")\\n\\nprint(b1[\"Y\"].shape, b1[\"c\"].shape)\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FcxU-mDXHY8X"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOUesxX5Ku5E"
      },
      "source": [
        "**2. Implement the neural network for training approximation of Hamiltonian function**\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PodXYaWSJ1d"
      },
      "source": [
        "The proposed model is formulated as \n",
        "$$\n",
        "\\tilde{F}(Y; \\theta) = \\eta((Z^{(K)})^T w + \\mu \\mathbf{1} ) \n",
        "$$\n",
        "where \n",
        "$$\n",
        "Z^{(k+1)} = Z^{(K)} + h  \\sigma(W_k Z^{(k)} + b_1 ) \\\\\n",
        "Z^{(0)} = \\hat{I} Y.\n",
        "$$\n",
        "Here is a\n",
        "\n",
        "$$\n",
        "\\hat{I} = \n",
        "\\begin{bmatrix}\n",
        "I_{d_0 \\times d_0} \\\\\n",
        "0\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "where the gradient is define as \n",
        "...\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kCPhfZlZUGM"
      },
      "source": [
        "def F_tilde(Y, th, d_0, d, K, h):\n",
        "    \n",
        "    Z = {}\n",
        "    I_d = np.identity(d)[:,:d_0]\n",
        "    Z[0] = I_d@Y\n",
        "\n",
        "    for k in range(K):\n",
        "        Z_hat = th[\"W\"][k]@Z[k]+th[\"b\"][k]\n",
        "        Z[k+1] = Z[k] + h*sigma(Z_hat, False)\n",
        "    \n",
        "    Upsilon = eta(Z[K].T@th[\"w\"]+th[\"mu\"])\n",
        "    \n",
        "    return Z, Upsilon \n",
        "\n",
        "\n",
        "def initialize_weights(d_0, d, K):\n",
        "    th = {}\n",
        "    \n",
        "    th[\"W\"] = np.zeros((K, d, d))\n",
        "    th[\"b\"] = np.zeros((K, d, 1))\n",
        "    \n",
        "    for i in range(K):\n",
        "        th[\"W\"][i] = np.identity(d)\n",
        "        th[\"b\"][i] = np.zeros((d, 1))\n",
        "            \n",
        "    th[\"w\"] = np.ones((d, 1 ))\n",
        "    th[\"mu\"] = np.zeros((1, 1))\n",
        "    \n",
        "    return th\n",
        "\n",
        "\n",
        "\n",
        "def sigma(x, derivative=False):   \n",
        "    if (derivative):\n",
        "        return 1 / np.cosh(x)**2 \n",
        "    return np.tanh(x)\n",
        "\n",
        "def eta(x, derivative=False, identity=False):\n",
        "    if identity==True:\n",
        "        if (derivative):\n",
        "            return np.ones(x.shape)\n",
        "        return x\n",
        "    else:\n",
        "        if (derivative):\n",
        "            return 0.25*(np.cosh(0.5*x) )**(-2)\n",
        "        return 0.5*np.tanh(0.5*x) + 0.5\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsS4_c-RZMM1"
      },
      "source": [
        "\n",
        "The cost function is on the form \n",
        "\n",
        "$$\n",
        "J(\\theta) = \\frac{1}{2} \\lVert  \\tilde{F} (Y; \\theta) - c \\rVert \n",
        "$$\n",
        "where $\\theta  = \\{ W_0, ..., W_{K-1}, b_0, ..., b_{K-1} , w, \\mu\\}$.\n",
        "\n",
        "However, to be able to optimize the parameters $\\theta$ can the gradient of $\\nabla J(\\theta)$ be formulated for the last layer like this\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "\\frac{\\partial J}{\\partial \\mu} &= \\eta '((Z^{(K)})^T w + \\mu \\mathbf{1} )^T (\\Upsilon -c) \\\\\n",
        "\\frac{\\partial J}{\\partial w} &= Z^{(K)} \\left [ (\\Upsilon - c) \\circ \\eta'((Z^{(K)})^T w + \\mu  \\right ]\n",
        "\\end{align}\n",
        "$$\n",
        "and similarly for the hidden layers,\n",
        "$$\n",
        "\\begin{align}\n",
        "P^{(K)} &= w \\cdot \\left [ (\\Upsilon - c) \\circ \\eta'\\left (  (Z^{(K)})^T w + \\mu \\ \\right) \\right ]^T  \\\\ \n",
        "P^{(k-1)} &= w \\cdot \\left [ (\\Upsilon - c) \\circ \\eta'\\left (  (Z^{(K)})^T w + \\mu \\ \\right) \\right ]^T  \\\\\n",
        "\\frac{\\partial J}{\\partial W_k} &= h \\left [ P^{(k+1)} \\circ \\sigma ' (W_k Z^{(k)}  + b_k) \\right ] (Z^{(k)})^T \\\\\n",
        "\\frac{\\partial J}{\\partial b_k} &= h \\left [ P^{(k+1)} \\circ \\sigma ' (W_k Z^{(k)} + b_k) \\right ] \\mathbf{1}.\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ElCD2r7hkhgh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zS8S6LqDkkML"
      },
      "source": [
        "\n",
        "def dJ_func(c, Y, th, d_0, d, K, h):\n",
        "    Z, Upsilon = F_tilde(Y, th, d_0, d, K, h)\n",
        "    I = Upsilon.shape[0]\n",
        "        \n",
        "    etahat = eta(Z[K].T@th[\"w\"] + th[\"mu\"]*np.ones(( I, 1)), derivative=True )\n",
        "        \n",
        "    P = np.zeros(( K+1, d, I))\n",
        "    \n",
        "    P[K] = np.outer(th[\"w\"], ( (Upsilon - c)* etahat).T)\n",
        "        \n",
        "    dJ_mu = etahat.T @(Upsilon - c)\n",
        "        \n",
        "    dJ_w = Z[K] @ ((Upsilon - c) * etahat)\n",
        "        \n",
        "    for k in range(K, 0, -1):\n",
        "        P[k-1] = P[k] + h*th[\"W\"][k-1].T @ (sigma(th[\"W\"][k-1]@Z[k-1]+np.outer(th[\"b\"][k-1],np.ones(I)), True) * P[k])\n",
        "            \n",
        "    dJ_W = np.zeros((K, d, d))\n",
        "    dJ_b = np.zeros((K, d, 1))\n",
        "        \n",
        "    for k in range(K):\n",
        "        dsigma = sigma(th[\"W\"][k]@Z[k]+np.outer(th[\"b\"][k],np.ones(I)),True)\n",
        "            \n",
        "        dJ_W[k] = h*(P[k+1]*dsigma) @ Z[k].T\n",
        "        dJ_b[k] = (h*(P[k+1]*dsigma) @ np.ones(I))[:,np.newaxis]\n",
        "    dJ = {}\n",
        "    dJ[\"w\"], dJ[\"mu\"], dJ[\"W\"], dJ[\"b\"] = dJ_w, dJ_mu, dJ_W, dJ_b\n",
        "    return dJ"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSmD03R5kwH_"
      },
      "source": [
        "To simplify input and output and make the training process more convinient are we introducing a linear scale function \n",
        "$$\n",
        "\\hat{x} = \\frac{(x + \\alpha )b - (x - \\beta)a }{b - a }\n",
        "$$\n",
        "\n",
        "and the inverse function\n",
        "\n",
        "$$\n",
        "x = \\frac{(\\hat{x}+ \\alpha ) b - (\\hat{x} - \\beta)a}{ \\beta -  \\alpha}\n",
        "$$\n",
        "\n",
        "where $b = \\max _{i,j} x$ and $a = \\min _{i,j} x$ such that all values in $ [a,b ] \\mapsto [\\alpha, \\beta]$. Usually will we choose $\\beta = 1$ and $\\alpha = 1$. In addition to this have we chosen to do scaling with no shift. Later in the project will we also have use for inversion of scale with no shift, this is in the context of finding the derivative. \n",
        "$$\n",
        "\\hat{x} = \\frac{x (b-a) }{\\beta - \\alpha }\n",
        "$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_MsdgkmsSgh"
      },
      "source": [
        "\n",
        "def scale(x, alpha=0, beta=1, returnParameters = False):\n",
        "    \n",
        "    a = np.min(x)\n",
        "    b = np.max(x)\n",
        "    \n",
        "    if returnParameters:        \n",
        "        return alpha, beta, a, b\n",
        "\n",
        "    else:\n",
        "        def  invscale(x):\n",
        "            return ((x + alpha)*b - (x - beta)*a) / (beta-alpha)\n",
        "        \n",
        "        return ( (b - x)*alpha + (x - a)*beta)/(b - a), invscale\n",
        "     \n",
        "def invscaleparameter(x, alpha, beta, a, b):\n",
        "    return ((x+alpha)*b - (x-beta)*a) / (beta-alpha)\n",
        "\n",
        "def invscaleparameter_no_shift(x, alpha, beta, a, b):\n",
        "    return x*(b-a)/(beta-alpha)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srOtimIQsWbx"
      },
      "source": [
        "**Optimization Algorithms**\n",
        "We will use two optimization algorithms in the project.\n",
        "\n",
        "1. **Gradient Descent**\n",
        "    $$\n",
        "    \\theta_{i+1} = \\theta_{i+1} - \\tau \\nabla J(\\theta^{(r)})\n",
        "    $$\n",
        "2. **Adams Algorithm**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0MSDfOEQLAES"
      },
      "source": [
        "\n",
        "\n",
        "def gradientDesent(K, th, dJ, tau):\n",
        "    \n",
        "    th[\"mu\"] = th[\"mu\"] - tau*dJ[\"mu\"]\n",
        "    th[\"w\"] = th[\"w\"] - tau*dJ[\"w\"]\n",
        "    \n",
        "    th[\"W\"] = th[\"W\"] -  tau*dJ[\"W\"]\n",
        "    th[\"b\"] = th[\"b\"] -  tau*dJ[\"b\"]\n",
        "    return th\n",
        "\n",
        "\n",
        "def adam_algebra(th, dJ, v, m, key, j, alpha =10**(-5)):\n",
        "        beta_1, beta_2 =  0.9, 0.999\n",
        "        epsilon =  10**(-8)\n",
        "    \n",
        "        g = dJ[key] \n",
        "        m[key] = beta_1*m[key] + (1- beta_1)*g\n",
        "        v[key] = beta_2*v[key] + (1 - beta_2)*(g*g)\n",
        "        mhat = m[key]/(1 - beta_1**(j+1))\n",
        "        vhat = v[key]/(1 - beta_2**(j+1))\n",
        "        th[key] -= alpha*mhat/(np.square(vhat) + epsilon)\n",
        "        #print(\"hat\",  vhat, v[key], mhat, m[key], j+1 )\n",
        "        return th, v, m\n",
        "    \n",
        "\n",
        "\n",
        "def train(c, d, d_0, K, h, Y, th, tau=0.0005, max_it=60, print_it=True, method=\"gd\", alpha =7.5*10**(-5)):\n",
        "    # compute Zk\n",
        "    err = np.inf\n",
        "    tol = 0.01\n",
        "    \n",
        "    itr = 0\n",
        "    Z, Upsilon = F_tilde(Y, th, d_0, d, K, h)\n",
        "    JJ = np.zeros(max_it+1)\n",
        "    err = J_func(Upsilon,c)\n",
        "    \n",
        "    JJ[0] = err\n",
        "    \n",
        "    # Adam parameters \n",
        "    m = {}\n",
        "    m[\"mu\"] = np.zeros(th[\"mu\"].shape)\n",
        "    m[\"w\"] = np.zeros(th[\"w\"].shape)\n",
        "    m[\"W\"] = np.zeros(th[\"W\"].shape)\n",
        "    m[\"b\"] = np.zeros(th[\"b\"].shape)\n",
        "    v = copy(m)\n",
        "    \n",
        "    while (itr < max_it ):\n",
        "        \n",
        "        Z, Upsilon = F_tilde(Y, th, d_0, d, K, h)\n",
        "        \n",
        "        \n",
        "        \n",
        "        if (method==\"gd\"):\n",
        "            dJ = dJ_func(c, Y, th, d_0, d, K, h)\n",
        "            th = gradientDesent(K, th, dJ, tau)\n",
        "        \n",
        "        elif (method==\"adam\"):\n",
        "            j = itr\n",
        "            \n",
        "            dJ = dJ_func(c, Y, th, d_0, d, K, h)\n",
        "            \n",
        "            th, v, m = adam_algebra(th, dJ, v, m, \"mu\", j, alpha)\n",
        "            th, v, m = adam_algebra(th, dJ, v, m, \"w\", j, alpha)\n",
        "            th, v, m = adam_algebra(th, dJ, v, m, \"W\", j, alpha)\n",
        "            th, v, m = adam_algebra(th, dJ, v, m, \"b\", j, alpha)\n",
        "            \n",
        "        else:\n",
        "            print(\"No optimization method\")\n",
        "        \n",
        "        err = J_func(Upsilon, c)  \n",
        "        \n",
        "        JJ[itr+1] = err\n",
        "        \n",
        "        itr += 1\n",
        "        \n",
        "        if(itr%600 == 0) and (print_it == True):\n",
        "            print(itr,err)\n",
        "        \n",
        "    return JJ , th\n",
        "        \n",
        "def stocgradient(c, d, d_0, K, h, Y, th, tau, max_it , bsize, sifts = 100):\n",
        "    \n",
        "    JJ = np.array([])\n",
        "    I = Y.shape[1]\n",
        "    totitr = int(I/bsize)\n",
        "    for siftnum in range(sifts):\n",
        "        #print(siftnum)\n",
        "        \n",
        "        indexes = np.array(range(I))\n",
        "        \n",
        "        np.random.shuffle(indexes)\n",
        "        \n",
        "        itr = 0\n",
        "        \n",
        "        Z, Upsilon = F_tilde(Y, th, d_0, d, K, h)\n",
        "        err = J_func(Upsilon, c)\n",
        "        JJ = np.append(JJ, err)\n",
        "        \n",
        "        while len(indexes) > 0:\n",
        "            \"\"\"\n",
        "            if (itr % 50 == 0):\n",
        "                print(siftnum,itr,totitr)\n",
        "            itr +=1\n",
        "            \"\"\"\n",
        "            if len(indexes) >= bsize:\n",
        "                #bsliceI = np.random.choice( indexes, bsize, False)\n",
        "                bsliceI = indexes[:bsize]\n",
        "                Yslice = Y[:,bsliceI]\n",
        "                cslice = c[bsliceI]\n",
        "                \n",
        "                dJJ, th = train(cslice, d, d_0, K, h, Yslice, th, tau, max_it)\n",
        "                #JJ = np.append(JJ,dJJ)\n",
        "                indexes = indexes[bsize:]\n",
        "                \n",
        "                \n",
        "            else:\n",
        "                Yslice = Y[:,indexes]\n",
        "                cslice = c[indexes]\n",
        "                \n",
        "                dJJ, th = train(cslice, d, d_0, K, h, Yslice, th, tau, max_it)\n",
        "                #JJ = np.append(JJ,dJJ)  \n",
        "                indexes = []\n",
        "            \n",
        "    return JJ, th\n",
        "\n",
        "\n",
        "def variablestocgradient(c, d, d_0, K, h, Y, th, tau, max_it, sifts):\n",
        "    \n",
        "    bsizes = np.array([10, 20, 40, 80, 160, 360])\n",
        "    JJ = np.array([])\n",
        "    \n",
        "    for bsize in bsizes:\n",
        "        dJJ, th = stocgradient(c, d, d_0, K, h, Y, th, tau, int(bsize/10) , bsize, sifts)\n",
        "        JJ = np.append(JJ,dJJ)\n",
        "    \n",
        "    return JJ, th\n",
        "\n",
        "def dF_tilde_y(y, h, th, d_0, d, K):\n",
        "    \n",
        "    Z, Upsilon = F_tilde(y, th, d_0, d, K, h)\n",
        "    \n",
        "    dz =  np.identity(d)[:,:d_0]    \n",
        "    for k in range(0,K):\n",
        "        dz =  h* sigma(th[\"W\"][k]@ Z[k] +  th[\"W\"][k], derivative = True)@(th[\"W\"][k] @dz) + dz     \n",
        "    dUpsilon = eta(Z[K].T @ th[\"w\"]  + th[\"mu\"], derivative=True )  @ (th[\"w\"].T@dz)\n",
        "    #print(\"zk\", Z[K])\n",
        "    return dUpsilon \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdoRivM2NrQ5"
      },
      "source": [
        "(a) Test the model by using the suggested functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIwq2aMhOFon",
        "outputId": "cd04e727-38af-4d2d-d1f6-3820c3bc0a4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        }
      },
      "source": [
        "\n",
        "def sqr_test():\n",
        "    b = generate_synthetic_batches(I,\"1sqr\")\n",
        "    Y = b[\"Y\"]\n",
        "\n",
        "    c, inv = scale(b[\"c\"])\n",
        "\n",
        "    d_0 = Y.shape[0]\n",
        "    d = d_0*2\n",
        "\n",
        "    th = initialize_weights(d_0, d, K)\n",
        "    JJ, th = train(c, d, d_0, K, h, Y, th, tau, max_it, method=\"gd\")\n",
        "    \n",
        "    x = np.linspace(-2, 2, I)\n",
        "    x = np.reshape(x,(1,len(x)))\n",
        "    y = 1/2 *x**2\n",
        "    \n",
        "    z, yhat = F_tilde(x, th, d_0, d, K, h)\n",
        "    yhat = inv(yhat)\n",
        "    yhat = yhat.T\n",
        "\n",
        "    plt.title(\"Square\")\n",
        "    plt.scatter(x.T,y.T)\n",
        "    plt.plot(x.T,yhat.T)\n",
        "    plt.show()\n",
        "    \n",
        "    plt.title(\"Square Cost function\")\n",
        "    plt.plot(JJ)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def cos_test():\n",
        "    high = 2*np.pi\n",
        "    low = -2*np.pi\n",
        "    b = generate_synthetic_batches(I,\"1cos\", low, high)\n",
        "\n",
        "    Y = b[\"Y\"]\n",
        "    c, inv = scale(b[\"c\"])\n",
        "\n",
        "    d_0 = Y.shape[0]\n",
        "    d = d_0*2\n",
        "\n",
        "    th = initialize_weights(d_0, d, K)\n",
        "    JJ, th = train(c, d, d_0, K, h, Y, th, tau, max_it, method=\"gd\")\n",
        "    \n",
        "    x = np.linspace(low, high, I)\n",
        "    x = np.reshape(x,(1,len(x)))\n",
        "    y = 1 - np.cos(x)\n",
        "    \n",
        "    z, yhat = F_tilde(x, th, d_0, d, K, h)\n",
        "    yhat = inv(yhat)\n",
        "    yhat = yhat.T\n",
        "\n",
        "    plt.title(\"cos\")\n",
        "    plt.plot(x.T,y.T, label=\"Neural Net\")\n",
        "    plt.plot(x.T,yhat.T, label=\"Training Data\")\n",
        "    plt.show()\n",
        "    \n",
        "    plt.title(\"Cos Cost function\")\n",
        "\n",
        "    plt.plot(JJ)\n",
        "    plt.show()\n",
        "\n",
        "K = 20\n",
        "h = 0.1\n",
        "I = 20\n",
        "tau = 0.01\n",
        "max_it = 10000\n",
        "\n",
        "high = 2*np.pi\n",
        "low = -2*np.pi\n",
        "b = generate_synthetic_batches(I,\"2sqr\", low, high)\n",
        "\n",
        "cos_test()\n",
        "sqr_test()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-5e1f7204dd9f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_synthetic_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mI\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"2sqr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhigh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m \u001b[0mcos_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0msqr_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-5e1f7204dd9f>\u001b[0m in \u001b[0;36mcos_test\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Y\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0md_0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'scale' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spXY0gxgVEq6"
      },
      "source": [
        "(b) Investigate systematically what are optimal choices for K, Ï„ , d, h and any\n",
        "other choices you need to make. Balance performance in the generalisation\n",
        "phase with time consumption of training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5Fb1mimJs38"
      },
      "source": [
        "**Tau Sensitivity**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfNOhcBwJkCh"
      },
      "source": [
        "\n",
        "\n",
        "# Chosen default values\n",
        "K = 20\n",
        "h = 0.1\n",
        "d_0 = 2\n",
        "d = 4\n",
        "I = 600\n",
        "max_it = 300\n",
        "tau = 0.1\n",
        "\n",
        "\n",
        "def tau_sensitivity(method=\"gd\"):\n",
        "           \n",
        "    I = 100     \n",
        "    b = generate_synthetic_batches(I)\n",
        "    \n",
        "    c, inv = scale(b[\"c\"])\n",
        "    Y = b[\"Y\"]\n",
        "    #Y = scale(b[\"Y\"])\n",
        "    d_0 = Y.shape[0]\n",
        "    \n",
        "    var = [ 2, 1, 0.5, 0.25, 0.1, 0.01]\n",
        "    it = np.arange(0,max_it+1)\n",
        "    \n",
        "    for i in range(len(var)):    \n",
        "        tau = var[i]\n",
        "        th = initialize_weights(d_0, d, K)\n",
        "        JJ,th = train(c, d, d_0, K, h, Y, th, tau=tau, max_it=max_it, method=method)\n",
        "        plt.plot(it, JJ, label=\"tau: \"+ str(tau))\n",
        "    \n",
        "    plt.title(\"Tau Sensitivity Analysis for \" + method)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    \n",
        "def tauI_sensitivity(I, method=\"gd\"):            \n",
        "    \n",
        "    b = generate_synthetic_batches(I)\n",
        "    \n",
        "    c, inv = scale(b[\"c\"])\n",
        "    Y = b[\"Y\"]\n",
        "    #Y = scale(b[\"Y\"])\n",
        "    d_0 = Y.shape[0]\n",
        "    \n",
        "    var = np.array([ 40, 20, 10, 5, 2, 0.2])/I\n",
        "    it = np.arange(0,max_it+1)\n",
        "    \n",
        "    for i in range(len(var)):    \n",
        "        tau = var[i]\n",
        "        th = initialize_weights(d_0, d, K)\n",
        "        JJ,th = train(c, d, d_0, K, h, Y, th, tau=tau, max_it=max_it, method=method)\n",
        "        plt.plot(it, JJ, label=\"tau: \"+ str(tau) + \"/I\")\n",
        "    \n",
        "    plt.title(\"Tau Sensitivity Analysis for \" + method + \", I: \"+str(I))\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "tau_sensitivity(method=\"gd\")\n",
        "I = [160, 320, 720, 1440, 2880, 5760]\n",
        "for i in I:\n",
        "    tauI_sensitivity(i)\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lu7F-HR1Jy8_"
      },
      "source": [
        "**Alpha Sensitivity**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-vkjCabJqMm"
      },
      "source": [
        "\n",
        "def alpha_sensitivity(method=\"adam\"):\n",
        "                \n",
        "    I = 100\n",
        "    b = generate_synthetic_batches(I)\n",
        "    \n",
        "    c, inv = scale(b[\"c\"])\n",
        "    Y = b[\"Y\"]\n",
        "    #Y = scale(b[\"Y\"])\n",
        "    d_0 = Y.shape[0]\n",
        "    \n",
        "    var = [0.75*10**-4, 0.5*10**-4, 0.35*10**-4, 0.75*10**-5, 0.5*10**-5, 0.25*10**-6]\n",
        "    it = np.arange(0,max_it+1)\n",
        "    \n",
        "    for i in range(len(var)):    \n",
        "        alpha = var[i]\n",
        "        th = initialize_weights(d_0, d, K)\n",
        "        JJ,th = train(c, d, d_0, K, h, Y, th, tau=tau, max_it=max_it, method=method, alpha=alpha)\n",
        "        plt.plot(it, JJ, label=\"alpha: \"+ str(alpha))\n",
        "    \n",
        "    plt.title(\"Alpha Sensitivity Analysis for \" + method)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    \n",
        "def alphaI_sensitivity(I, method=\"adam\"):            \n",
        "    \n",
        "    b = generate_synthetic_batches(I)\n",
        "    \n",
        "    c, inv = scale(b[\"c\"])\n",
        "    Y = b[\"Y\"]\n",
        "    d_0 = Y.shape[0]\n",
        "    \n",
        "    var = np.array( [10**-1, 10**-2, 10**-3, 10**-4, 10**-5, 10**-6 ])#/I\n",
        "    it = np.arange(0,max_it+1)\n",
        "    \n",
        "    for i in range(len(var)):    \n",
        "        alpha = var[i]\n",
        "        th = initialize_weights(d_0, d, K)\n",
        "        JJ,th = train(c, d, d_0, K, h, Y, th, tau=tau, max_it=max_it, method=method, alpha=alpha)\n",
        "        plt.plot(it, JJ, label=\"alpha: \"+ str(alpha) )\n",
        "    \n",
        "    plt.title(\"ADAM Sensitivity - \" + \"I: \"+str(I))\n",
        "    plt.xlabel(\"Iteration\")\n",
        "    plt.ylabel(\"Cost function\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "I = [160, 320, 720, 1440, 2880, 5760]\n",
        "for i in I:\n",
        "    alphaI_sensitivity(i)\n",
        "\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSCYZtYBJ_Q3"
      },
      "source": [
        "**h Sensitivity**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O58x1zChNtGe"
      },
      "source": [
        "\n",
        "def h_sensitivity(method=\"gd\"):\n",
        "    I = 300\n",
        "    b = generate_synthetic_batches(I)\n",
        "    \n",
        "    c, inv = scale(b[\"c\"])\n",
        "    Y = b[\"Y\"]\n",
        "    #Y = scale(b[\"Y\"])\n",
        "    d_0 = Y.shape[0]\n",
        "    \n",
        "    var = var = [ 0.14, 0.12, 0.1, 0.07, 0.05, 0.01]\n",
        "    it = np.arange(0,max_it+1)\n",
        "    \n",
        "    for i in range(len(var)):    \n",
        "        h = var[i]\n",
        "        th = initialize_weights(d_0, d, K)\n",
        "        JJ,th = train(c, d, d_0, K, h, Y, th, tau=tau, max_it=max_it, method=method)\n",
        "        plt.plot(it, JJ, label=\"h: \"+ str(var[i]))\n",
        "    \n",
        "    plt.title(\"h Sensitivity Analysis for \" + method)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "h_sensitivity(method=\"gd\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjsPTvJWKI78"
      },
      "source": [
        "**I-sensitivity**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5iSsxhkN1cN"
      },
      "source": [
        "\n",
        "def I_sensitivity(method=\"gd\"):\n",
        "    max_it = 3000             \n",
        "    var = var = [5, 10, 15, 20, 40, 80, 160, 320]\n",
        "    it = np.arange(0,max_it+1)\n",
        "    \n",
        "    for i in range(len(var)):    \n",
        "        I = var[i]\n",
        "        b = generate_synthetic_batches(I)\n",
        "        c, inv = scale(b[\"c\"])\n",
        "        Y = b[\"Y\"]\n",
        "        #Y = scale(b[\"Y\"])\n",
        "        d_0 = Y.shape[0]\n",
        "    \n",
        "        print(\"I:\", I)\n",
        "        th = initialize_weights(d_0, d, K)\n",
        "        JJ,th = train(c, d, d_0, K, h, Y, th, tau=tau, max_it=max_it, print_it=False, method=method)\n",
        "        JJ = JJ/JJ[0]\n",
        "        plt.plot(it, JJ, label=\"I: \"+ str(var[i]))\n",
        "    \n",
        "    #plt.yscale(\"log\")\n",
        "    plt.title(\"I Sensitivity Analysis for \" + method)\n",
        "    plt.yscale(\"log\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "I_sensitivity(method=\"gd\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51B3HXphKQE3"
      },
      "source": [
        "**d-sensitivity**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AS3-TqQxNxDf"
      },
      "source": [
        "\n",
        "\n",
        "def d_sensitivity(method=\"gd\"):\n",
        "    I = 300         \n",
        "    b = generate_synthetic_batches(I)\n",
        "    \n",
        "    c, inv = scale(b[\"c\"])\n",
        "    Y = b[\"Y\"]\n",
        "    #Y = scale(b[\"Y\"])\n",
        "    d_0 = Y.shape[0]\n",
        "    \n",
        "    var = var = [ 2, 3, 4, 5, 6, 7, 8, 10 ]\n",
        "    it = np.arange(0,max_it+1)\n",
        "    \n",
        "    for i in range(len(var)):    \n",
        "        d = var[i]\n",
        "        th = initialize_weights(d_0, d, K)\n",
        "        JJ,th = train(c, d, d_0, K, h, Y, th, tau=tau, max_it=max_it, method=method)\n",
        "        plt.plot(it, JJ, label=\"d: \"+ str(var[i]))\n",
        "    \n",
        "    plt.title(\"d Sensitivity Analysis for \" + method)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "d_sensitivity(method=\"gd\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQrLZ8iJLAjE"
      },
      "source": [
        "**K-Sensitivity**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGZ6Cu6jK9_L"
      },
      "source": [
        "\n",
        "def K_sensitivity(method=\"gd\"):\n",
        "    I = 300         \n",
        "    b = generate_synthetic_batches(I)\n",
        "    \n",
        "    c, inv = scale(b[\"c\"])\n",
        "    Y = b[\"Y\"]\n",
        "    #Y = scale(b[\"Y\"])\n",
        "    d_0 = Y.shape[0]\n",
        "    \n",
        "    var = var = [ 4, 6, 10, 14, 17, 20, 30]\n",
        "    it = np.arange(0,max_it+1)\n",
        "    \n",
        "    for i in range(len(var)):    \n",
        "        K = var[i]\n",
        "        th = initialize_weights(d_0, d, K)\n",
        "        JJ,th = train(c, d, d_0, K, h, Y, th, tau=tau, max_it=max_it, method=method)\n",
        "        plt.plot(it, JJ, label=\"K: \"+ str(var[i]))\n",
        "    \n",
        "    plt.title(\"K Sensitivity Analysis for \" + method)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "K_sensitivity(method=\"gd\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSf0GGCDXlT2"
      },
      "source": [
        "(c) Train the model for the case of data given (with unknown Hamiltonian func-\n",
        "tion)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQNtdoYPXDVi"
      },
      "source": [
        "\n",
        "def train_uknown():\n",
        "    K = 20\n",
        "    h = 0.1\n",
        "    I = 80\n",
        "    max_it = 1\n",
        "    sifts = 110\n",
        "    tau = 0.1\n",
        "    \n",
        "    batches = import_batches()\n",
        "    batch1 = batches[0]\n",
        "    antB = 40\n",
        "    testbatch = batches[antB-1]\n",
        "    \n",
        "    Y = batch1[\"Y_q\"]\n",
        "    d_0 = Y.shape[0]\n",
        "    d = d_0*2\n",
        "    \n",
        "    th = initialize_weights(d_0, d, K)\n",
        "    JJ = np.array([])\n",
        "\n",
        "    bigbatch = {}\n",
        "    bigbatch[\"Y\"] = np.array([[],[],[]])\n",
        "    bigbatch[\"c\"] = np.array([])\n",
        "    \n",
        "    for i in range(antB):\n",
        "        batch = batches[i]\n",
        "        bigbatch[\"Y\"] = np.append(bigbatch[\"Y\"],batch[\"Y_q\"],1)\n",
        "        bigbatch[\"c\"] = np.append(bigbatch[\"c\"],batch[\"c_q\"])\n",
        "        \n",
        "    Y = bigbatch[\"Y\"]\n",
        "    c,inv = scale(bigbatch[\"c\"][:,np.newaxis])\n",
        "    \n",
        "    JJ, th = stocgradient(c, d, d_0, K, h, Y, th, tau, 1 , 40, sifts)\n",
        " \n",
        "    plt.plot(JJ)\n",
        "    plt.yscale(\"log\")\n",
        "    plt.show()\n",
        "    \n",
        "    tY = testbatch[\"Y_q\"]\n",
        "    tc,invscale = scale(testbatch[\"c_q\"])\n",
        "    \n",
        "    z, yhat = F_tilde(tY, th, d_0, d, K, h)\n",
        "    \n",
        "    y = invscale(yhat)\n",
        "    ic = invscale(tc)\n",
        "    \n",
        "    plt.plot(y)\n",
        "    plt.plot(ic)\n",
        "    plt.show()\n",
        "    \n",
        "    th_file = open(\"weights.pkl\", \"wb\")\n",
        "    pickle.dump(th, th_file)\n",
        "    th_file.close()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}